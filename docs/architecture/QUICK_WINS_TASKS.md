# üöÄ QUICK WINS TASKS
## **Immediate High-Impact, Low-Effort Implementation**

### **üéØ PRIORITY ORDER: Foundation First**

**Strategy**: Deploy CPU models first ‚Üí Test locally ‚Üí Integrate with existing system ‚Üí Scale up

---

## **‚ö° PHASE 0: CPU MODEL DEPLOYMENT (Day 1)**
### **Immediate Foundation - Deploy Before Everything Else**

#### **ü§ñ TASK 0.1: Deploy Local CPU Models (2-3 hours)**

**Why First**: All other tasks depend on having CPU models available for cost optimization.

**Scope**: Get 3 essential CPU models running locally
**Files**: `services/cpu-models/`
**Priority**: **CRITICAL - DO FIRST**

**Implementation Steps**:

**Step 1: Create CPU Models Directory**
```bash
mkdir -p services/cpu-models
cd services/cpu-models
```

**Step 2: Create Docker Compose for CPU Models**
```yaml
# services/cpu-models/docker-compose.yml
version: '3.8'
services:
  # FLAN-T5 Small - Text generation and step planning
  flan-t5-small:
    image: huggingface/text-generation-inference:latest
    ports: ["8001:80"]
    environment:
      - MODEL_ID=google/flan-t5-small
      - MAX_INPUT_LENGTH=512
      - MAX_TOTAL_TOKENS=1024
    volumes:
      - flan_t5_cache:/data
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  # DistilBERT - Classification and skill detection  
  distilbert-classifier:
    image: huggingface/transformers-pytorch-cpu
    ports: ["8002:8000"]
    environment:
      - MODEL_NAME=distilbert-base-uncased
      - TASK=classification
      - MAX_LENGTH=256
    volumes:
      - distilbert_cache:/cache
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # CodeBERT - Code analysis and validation
  codebert-analyzer:
    image: huggingface/transformers-pytorch-cpu
    ports: ["8003:8000"]
    environment:
      - MODEL_NAME=microsoft/codebert-base
      - TASK=code-analysis
      - MAX_LENGTH=512
    volumes:
      - codebert_cache:/cache
    deploy:
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 768M

volumes:
  flan_t5_cache:
  distilbert_cache:
  codebert_cache:
```

**Step 3: Create Model API Wrapper**
```javascript
// services/cpu-models/src/modelApi.js
const express = require('express');
const app = express();

app.use(express.json());

// FLAN-T5 endpoint
app.post('/flan-t5/generate', async (req, res) => {
  try {
    const { prompt, max_tokens = 256 } = req.body;
    
    const response = await fetch('http://localhost:8001/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        inputs: prompt,
        parameters: {
          max_new_tokens: max_tokens,
          temperature: 0.7,
          do_sample: true
        }
      })
    });
    
    const result = await response.json();
    res.json({ text: result.generated_text, model: 'flan-t5-small' });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// DistilBERT classification endpoint
app.post('/distilbert/classify', async (req, res) => {
  try {
    const { text, labels } = req.body;
    
    const response = await fetch('http://localhost:8002/classify', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        inputs: text,
        parameters: { candidate_labels: labels }
      })
    });
    
    const result = await response.json();
    res.json({ labels: result.labels, scores: result.scores });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// CodeBERT analysis endpoint
app.post('/codebert/analyze', async (req, res) => {
  try {
    const { code, language } = req.body;
    
    const response = await fetch('http://localhost:8003/analyze', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        inputs: code,
        parameters: { language }
      })
    });
    
    const result = await response.json();
    res.json({ analysis: result, model: 'codebert' });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.listen(8000, () => {
  console.log('CPU Models API running on port 8000');
});
```

**Step 4: Deploy and Test**
```bash
# Deploy CPU models
docker-compose up -d

# Test FLAN-T5
curl -X POST http://localhost:8000/flan-t5/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Break down this task: Build a login system", "max_tokens": 128}'

# Test DistilBERT
curl -X POST http://localhost:8000/distilbert/classify \
  -H "Content-Type: application/json" \
  -d '{"text": "Create user authentication system", "labels": ["security", "database", "api", "frontend"]}'

# Test CodeBERT
curl -X POST http://localhost:8000/codebert/analyze \
  -H "Content-Type: application/json" \
  -d '{"code": "function login(user, pass) { return auth.verify(user, pass); }", "language": "javascript"}'
```

**Acceptance Criteria**:
- ‚úÖ All 3 CPU models respond to API calls
- ‚úÖ Response time < 2 seconds for each model
- ‚úÖ Memory usage < 5GB total
- ‚úÖ Models auto-restart on failure

**Time Estimate**: 2-3 hours
**Impact**: Immediate 90% cost reduction for AI tasks

---

## **‚ö° QUICK WIN TASKS (Day 1-2)**
### **High-Impact, Low-Effort Wins**

#### **üîß TASK QW-1: CPU Model Integration in A2A Service (1 hour)**

**Scope**: Connect existing A2A service to CPU models
**Files**: `services/design-microservice/src/services/a2aService.js`

**Implementation**:
```javascript
// Add to a2aService.js constructor
class A2AService {
  constructor() {
    // ... existing code ...
    
    // Add CPU model client
    this.cpuModels = {
      flanT5: 'http://localhost:8000/flan-t5/generate',
      distilbert: 'http://localhost:8000/distilbert/classify',
      codebert: 'http://localhost:8000/codebert/analyze'
    };
  }

  // Add CPU-first routing
  async routeTask(task) {
    const complexity = this.calculateComplexity(task);
    
    if (complexity < 0.5) {
      // Use CPU models for simple tasks
      return await this.executeCPUTask(task);
    } else {
      // Fallback to existing Gemini for complex tasks
      return await this.executeWithAgent(selectedAgent, task);
    }
  }

  async executeCPUTask(task) {
    const response = await fetch(this.cpuModels.flanT5, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        prompt: this.buildCPUPrompt(task),
        max_tokens: 256
      })
    });
    
    const result = await response.json();
    return this.formatCPUResponse(result, task);
  }

  calculateComplexity(task) {
    const wordCount = task.content.split(' ').length;
    const hasCode = /function|class|import|const/.test(task.content);
    const hasArchitecture = /architecture|system|design/.test(task.content);
    
    let score = 0;
    if (wordCount > 50) score += 0.2;
    if (wordCount > 100) score += 0.2;
    if (hasCode) score += 0.3;
    if (hasArchitecture) score += 0.3;
    
    return Math.min(score, 1.0);
  }
}
```

**Acceptance**: CPU models handle 60%+ of requests, cost reduction visible immediately
**Time**: 1 hour

---

#### **üé® TASK QW-2: Add Model Selection UI (30 minutes)**

**Scope**: Show which model is being used in the UI
**Files**: `web/src/components/canvas/AgentPanel.tsx`

**Implementation**:
```typescript
// Add model indicator to AgentPanel
const [currentModel, setCurrentModel] = useState<string>('');

// In handleSendMessage function, add:
const modelUsed = response.data.model || 'cpu-model';
setCurrentModel(modelUsed);

// Add to JSX:
<div className="model-indicator">
  <span className={`model-badge ${modelUsed.includes('cpu') ? 'cpu' : 'llm'}`}>
    {modelUsed.includes('cpu') ? 'üñ•Ô∏è CPU Model' : 'üß† LLM'}
  </span>
  {modelUsed.includes('cpu') && <span className="cost-savings">üí∞ $0 cost</span>}
</div>
```

**Acceptance**: Users can see which model handled their request
**Time**: 30 minutes

---

#### **üìä TASK QW-3: Add Simple Cost Tracking (45 minutes)**

**Scope**: Track and display cost savings from CPU model usage
**Files**: `services/design-microservice/src/services/costTracker.js`

**Implementation**:
```javascript
class CostTracker {
  constructor() {
    this.costs = {
      cpuRequests: 0,
      llmRequests: 0,
      totalSaved: 0
    };
  }

  trackRequest(modelType, complexity) {
    if (modelType.includes('cpu')) {
      this.costs.cpuRequests++;
      this.costs.totalSaved += 0.25; // Avg LLM cost saved
    } else {
      this.costs.llmRequests++;
    }
  }

  getCostSummary() {
    const totalRequests = this.costs.cpuRequests + this.costs.llmRequests;
    const cpuPercentage = (this.costs.cpuRequests / totalRequests) * 100;
    
    return {
      cpuUsage: `${cpuPercentage.toFixed(1)}%`,
      totalSaved: `$${this.costs.totalSaved.toFixed(2)}`,
      requestsHandled: totalRequests
    };
  }
}
```

**Acceptance**: Dashboard shows cost savings in real-time
**Time**: 45 minutes

---

#### **‚ö° TASK QW-4: Template-Based Quick Responses (1 hour)**

**Scope**: Handle common queries with pre-built templates (no AI needed)
**Files**: `services/design-microservice/src/services/templateResponses.js`

**Implementation**:
```javascript
class TemplateResponses {
  constructor() {
    this.templates = {
      'login system': {
        response: 'I\'ll help you design a secure login system with authentication and session management.',
        canvasUpdate: {
          nodes: [
            { id: 'user-auth', type: 'service', data: { label: 'Authentication Service' } },
            { id: 'user-db', type: 'database', data: { label: 'User Database' } },
            { id: 'session-store', type: 'cache', data: { label: 'Session Store' } }
          ],
          edges: [
            { id: 'auth-db', source: 'user-auth', target: 'user-db' },
            { id: 'auth-session', source: 'user-auth', target: 'session-store' }
          ]
        }
      },
      'api design': {
        response: 'I\'ll create a RESTful API design with proper endpoints and documentation.',
        canvasUpdate: {
          nodes: [
            { id: 'api-gateway', type: 'service', data: { label: 'API Gateway' } },
            { id: 'auth-service', type: 'service', data: { label: 'Auth Service' } },
            { id: 'business-logic', type: 'service', data: { label: 'Business Logic' } }
          ]
        }
      }
    };
  }

  findTemplate(query) {
    const lowercaseQuery = query.toLowerCase();
    
    for (const [key, template] of Object.entries(this.templates)) {
      if (lowercaseQuery.includes(key)) {
        return template;
      }
    }
    
    return null;
  }
}
```

**Acceptance**: Common queries get instant responses (0ms, $0 cost)
**Time**: 1 hour

---

#### **üîÑ TASK QW-5: Add CPU Model Health Check (30 minutes)**

**Scope**: Monitor CPU model availability
**Files**: `services/cpu-models/healthcheck.js`

**Implementation**:
```javascript
// Simple health check endpoint
app.get('/health', async (req, res) => {
  const health = {
    status: 'healthy',
    models: {},
    timestamp: new Date().toISOString()
  };

  try {
    // Test FLAN-T5
    const flanResponse = await fetch('http://localhost:8001/health');
    health.models.flanT5 = flanResponse.ok ? 'healthy' : 'unhealthy';

    // Test DistilBERT
    const distilResponse = await fetch('http://localhost:8002/health');
    health.models.distilbert = distilResponse.ok ? 'healthy' : 'unhealthy';

    // Test CodeBERT
    const codeResponse = await fetch('http://localhost:8003/health');
    health.models.codebert = codeResponse.ok ? 'healthy' : 'unhealthy';

    res.json(health);
  } catch (error) {
    health.status = 'unhealthy';
    health.error = error.message;
    res.status(500).json(health);
  }
});
```

**Acceptance**: Health endpoint shows all model statuses
**Time**: 30 minutes

---

## **üìà IMMEDIATE IMPACT METRICS**

### **After CPU Model Deployment (Day 1)**
- **Cost Reduction**: 60-80% of requests use $0 CPU models
- **Response Time**: CPU models respond in 500ms-2s
- **Reliability**: Self-hosted models, no external API dependencies
- **Scalability**: Can handle 1000+ concurrent requests

### **Quick Wins Summary (Day 1-2)**
- **Total Time**: 5.5 hours
- **Cost Savings**: $0.15-0.25 per request saved
- **User Experience**: Faster responses for common queries
- **Infrastructure**: Robust, self-hosted AI capability

---

## **üéØ EXECUTION ORDER (Day 1)**

**Morning (3 hours)**:
1. ‚úÖ **Deploy CPU Models** (Task 0.1) - 2-3 hours
2. ‚úÖ **Test All Models** - 30 minutes

**Afternoon (2.5 hours)**:
3. ‚úÖ **Integrate with A2A Service** (Task QW-1) - 1 hour
4. ‚úÖ **Add Template Responses** (Task QW-4) - 1 hour  
5. ‚úÖ **Add Model UI Indicator** (Task QW-2) - 30 minutes

**Evening (1 hour)**:
6. ‚úÖ **Add Cost Tracking** (Task QW-3) - 45 minutes
7. ‚úÖ **Add Health Check** (Task QW-5) - 15 minutes

**End of Day 1**: 
- CPU models handling 60%+ of requests
- Immediate cost savings visible
- Foundation ready for advanced features

This approach gives you immediate wins and cost savings from Day 1, while building the foundation for the full streaming architecture! üöÄ 